[TOC]

# 神经网络

## 神经网络的输入过程

​		用一张图片来举例，一张图片的RGB图像可形成三个矩阵，最后将这三个矩阵全部转化为一个向量，而这一个向量（特征向量），包括了这张图片RGB三个通道的每一个数值，每一个数值都被称作特征，而将这个特征向量输入神经网络当中进行预测，通过各种复杂的计算进行预测

![image-20210202191618694](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202191618.png)

## 神经网络的预测过程

* 逻辑回归

​		预测的过程其实只是基于一个简单的公式：z = dot(w.T,x) + b  这个公式的使用如图：

![image-20210202193038302](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202193038.png)

![image-20210202193301090](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202193301.png)

***

结果值为z，x为其中的特征，w为每个特征对应的权重，b为直接影响计算结果的阈值，这个公式为逻辑回归公式

* 激活函数

但是在实际的神经网络当中，为了提高神经网络的学习性，我们不能直接使用逻辑回归，还需要使用激活函数来提高神经网络的 “智商”，以下为一个最简单的激活函数：

![image-20210202205508867](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202205509.png)

将结果映射到【0，1】之间，在该函数当中，逻辑回归当中的z值作为横坐标，当z越大，则得到的纵坐标y也越大，而将y作为最后预测的结果。

## 神经网络预测准确的判断过程

* 损失函数

当我们进行预测之后，还需要进行预测准确的检测，我们一般运用损失函数来进行准确预测的计算，通俗来说，损失函数大概就是让预测结果和实际结果之间 “做差”，得到 “差值”，而值越大也就意味着这个样本的预测偏差度越高。

![image-20210202221049923](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202221049.png)

![image-20210202220938254](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202221022.png)

* 成本函数

而对于单个样本我们定义了损失函数，对于整个训练集我们也定义了一个损失函数，也可称为——成本函数，以下便为成本函数：

![image-20210202222232722](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202222232.png)

* 损失函数的定义

![image-20210202222409225](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202222409.png)

![image-20210202222437452](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202222437.png)

## 神经网络的学习过程

* 梯度下降的介绍

基于前面的公式，我们可以得到一个结论，预测的是否准确需要由w和b的合适程度来决定（即权重和阈值来决定），而在神经网络当中，不断地更改w和b的值（一步步的精确）使得神经网络中损失函数的结果变得更小这个过程，就是神经网络的学习，也被称为梯度下降。

![image-20210202230507706](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202230507.png)

（之前学的逻辑回归以及损失函数的公式）

![image-20210202232844384](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202232844.png)

![image-20210202232943411](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202232943.png)

* 梯度下降算法的具体解释

![image-20210202235715079](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210202235715.png)

（这个dw相当于在损失函数当中的W-J坐标系当中的斜率，J的变化量除以W的变化量）

而在运算当中，若让w每次变化数值过大，则有可能出现一种情况——该模型永远无法达到J的最低值，而r的选择就很重要了，选择一个合适的r可以提高准确率和降低运算复杂度，当然，学习率也是可以改变的，所以也有一个概念——学习率衰减

![image-20210203003730018](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210203003730.png)

![image-20210203003807661](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210203003807.png)

![image-20210203003840162](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210203003840.png)

当然，在这篇入门笔记当中，这个概念只涉及到此，待之后深入了解会继续深究

## 推展神经网络的整体计算过程

*神经网络要整理计算图，理清思路，方便进行计算推导*

* 神经网络计算过程的概念

![image-20210203233121892](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210203233121.png)

* 关于偏导数的计算（链式法则）

在神经网络的传播当中一般有多个计算式，而我们所需的用于反向传播的偏导数的计算，应该为每个次级计算式的导数的乘积（链式法则）

* 例子：

  ![image-20210204011551869](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210204011551.png)

  ![image-20210204011521716](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210204011521.png)

  ![image-20210204013127638](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210204013127.png)

  ![image-20210204011759763](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210204011759.png)

*以上的计算都只针对某个特定的函数来说，在不同的神经网络当中，有着不同的函数，例如说损失函数就有（0-1损失）（损失感知）（交叉熵增损失函数），还有各种激活函数，不同的函数构成了不同的前向传播*

当我们得到dw（i），db这些偏导数之后，就可以使用他们来进行反向传播梯度下降，再用更新的参数值再次开始下一次的输入。

* 多个样本条件下的偏导数计算

  *（譬如在识别猫的神经网络当中，一张猫的图片就是一个训练样本）*

  先看之前学的成本，成本其实就是多个样本的损失平均值——m个样本的损失累加起来再除以m就是成本，那么同理来说，多个样本的偏导数就是每个样本的偏导数的平均值

* 在代码层面实现神经网络（基础）

  *必须要向量化代码，用向量化可充分发挥计算机的并行计算能力，而不是for循环的单指令流，可大大提高运算速度（约300倍）*

  ![image-20210204150352956](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210204150353.png)





