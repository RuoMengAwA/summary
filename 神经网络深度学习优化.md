[TOC]

# 前言

在之前的学习中，我大概已经了解基础的深度学习程序的概念和初步构建，但在实战当中会出现各种问题，接下来就到了该篇笔记的主要内容，进阶，为实战奠基。

# 配置数据集

有多需要人工智能就有多需要数据，数据量的大小直接影响深度学习的性能，事实证明小数据使用深度学习往往会造成过拟合，使用小数据集在越复杂的问题当中越容易出现这些问题，使得你的模型虽然已经实际取得了结果，但却仅限适用于你训练的集合

数据集一般分为3种（训练集/验证集/测试集）

对于一个新项目的数据处理过程，一般就是不断的迭代训练（训练集）-验证（验证集）-调整超参数-再训练-再验证----（得到一个合适的超参数）-测试（用测试集进行测试）

数据集的划分获取，尽量保证在训练，验证和测试训练集的数据来源是一致的，或者保证这三个数据集都等量混合了来自不同来源的数据，这样才可以保证获得一个比较好的验证结果。

# 欠拟合和过拟合

## 概念和前提条件

拟合度可以被理解为训练好的模型与训练模型的匹配程度，若匹配度不高就称为欠拟合（也称高偏差），若匹配度过高就称为过拟合（也称高方差），过拟合的表现之一就是训练好的模型对训练数据集的准确度高但是对验证数据集的预测准确度却很低。

还有一种情况是既过拟合又欠拟合，如图：

![image-20210301005923186](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301005923.png)

* 而知道概念后还需要保证达到两个前提条件才可保证判定的准确：

![image-20210301010103430](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301010103.png)

## 解决欠拟合

欠拟合的问题比较好处理，且欠拟合会在训练过程当中就表现出来，更容易去控制，相较来说，过拟合会比较难发现也比较难处理

![image-20210301010409915](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301010409.png)

## 解决过拟合

![image-20210301010713505](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301010713.png)

![image-20210301010902039](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301010902.png)

* 较为常用的解决过拟合方法

![image-20210301011620592](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301011620.png)

![image-20210301011641436](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301011641.png)

![image-20210301011652825](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301011652.png)

## 同时解决欠/过拟合的方法

在使用了合适的正则化之后，使用更大的神经网络（也需要提高计算机的能力来计算更大的神经网络），获取更多的训练数据（需要时间，金钱）

* 正则化

而在这些限制条件之后，最容易实现的就只剩下正则化了（但正则化主要还是用来解决过拟合，如果不是同时过拟合又欠拟合，不可以单独使用正则化来解决欠拟合！！！），正则化又被称为规则化，规则化简单来说，就是给训练的目标函数加上一些规则限制，让他们不要自我膨胀

* L2正则化

![image-20210301014040231](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301014040.png)

先用单神经元的正则化来做例子：

![image-20210301153107871](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301153108.png)

而在单神经元正则化之后，我们就可以更好理解和其类似的多神经元正则化了：

![image-20210301155722611](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301155722.png)

在成本函数后面加上尾巴之后，就可以在计算偏导数的时候加上尾巴了

![image-20210301190350498](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301190350.png)

![image-20210301192644697](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301192644.png)

![image-20210301192813961](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301192814.png)

* dropout（失活）

使用领域：多使用于图像识别领域（因为图像识别领域比较缺少样本）

基本原理：随机删除神经元，对每一层设置一个概率数，用其来控制每一层保留多少神经元，而每个样本导入模型的时候都会随机删除一次（之后会还原）

缺点（使用需注意）：

![image-20210301203345052](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301203345.png)

实现：

![image-20210301194819616](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301194819.png)

![image-20210301194848225](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301194848.png)

如：

![image-20210301200333693](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301200333.png)

此时再将得到的 d2 * a2 => a2此时的a2就相当于被删除后的一层神经网络，实现了dropout，但这只是传统的dropout，要实现inverted dropout：

![image-20210301200910433](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301200910.png)

那转念一想，你这一下乘一下除，那还顶个球用啊，你这怎么解决过拟合啊，没错，我刚刚就是这样想的，然后以下有两个原因让我明白了：

1.使用dropout可以使得神经元减少，神经网络简化之后产生的效果对过拟合有奇效。

2.如图：

![image-20210301202918868](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301202919.png)

# 数据增强

> ***数据就是财富，我们拥有的不多，但却可以利用仅有的，不断创造更多财富***

为解决数据集不足的问题，我们可以对各类数据进行不同的处理：

* 对于图片：

1.垂直，水平翻转

2.旋转截取部分

3.轻微扭曲，旋转（对数字）

4.高斯噪声，丢弃黑色矩阵块，产生彩色噪声（噪声增强）

5.颜色扰动——增加或减少某些颜色分量或者改变颜色通道的顺序

![image-20210301221821984](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301221822.png)

![image-20210301221844811](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301221844.png)

![image-20210301222301133](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222301.png)

![image-20210301222313518](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222313.png)

* 对于音频：

![image-20210301222413325](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222413.png)

* 对于文本：

1.回译

2.随机词替换增强

3.EDA

![image-20210301222457459](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222457.png)

实例：

![image-20210301222518788](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222518.png)

![image-20210301222704236](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222704.png)

![image-20210301222723384](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222723.png)

实例：

![image-20210301222737097](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301222737.png)

# 对输入特征进行规范（归一）化

***对数据进行处理，使其满足某种规范，以方便之后神经网络对输入特征进行计算***

* 一种常见的规范化方法：

第一步：使得数据集的平均值为0 (先求当前的平均值，再使每一个样本减去当前的平均值)

第二步：使数据集的方差为1

![image-20210301223930504](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301223930.png)

![image-20210301223949034](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301223949.png)

![image-20210301224005415](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210301224005.png)







