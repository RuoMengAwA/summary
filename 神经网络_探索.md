[TOC]



 # 神经网络

**之前的神经网络初探的笔记记录的是最简单的单神经元的神经网络，现在，该开始探索稍高级一点的浅层神经网络了**

## 回顾单神经元神经网络

![image-20210210193430675](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210210193430.png)

![image-20210210193453657](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210210193453.png)

## 多神经元神经网络

***多神经网络也可以看成多个单神经网络***

### 浅层

* 介绍

![image-20210212223923931](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210212223924.png)

![image-20210212235636734](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210212235636.png)

* 计算

![image-20210213000058532](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210213000058.png)

![image-20210213000153768](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210213000153.png)

* 隐藏层的神经元数量

![image-20210227030351107](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210227030351.png)

#### 激活函数

* 激活函数的必要性和常见的激活函数

训练神经网络目的在于，如何找到一个函数来解决一个特定的问题，经过训练之后得到一个适合解决该问题的w和b，而在现实生活当中，解决的问题不一定是线性问题，而且神经网络在纯线性条件下，无论有几层神经网络都相当于一层神经网络，而加入激活函数套在前面的线性函数前面那么整个函数就变成了非线性函数了，而在此基础上，只要神经网络的层数足够多，即叠加起来的非线性函数越多，即可表示越复杂的曲线，那么只要层数足够，任何问题都可以解决。

那么常见的激活函数有哪些呢：

1.sigmoid函数

![image-20210228143153987](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228143154.png)

![image-20210228140349578](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228140349.png)

该函数使用频率较少，因为它有个升级版函数——tanh函数，但是该函数相较于它的升级版更适合用于二元分类神经网络的输出层，因为其在二元分类的输出值恰好为0-1之间，恰好可以表示预测的概率。

2.tanh函数

![image-20210228143212505](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228143212.png)

![image-20210228140744407](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228140744.png)

该函数拥有sigmoid函数的形状，但是整体下移，所以其输出值平均为0而不是sigmoid的0.5，故在神经网络当中，输出值在下一层的计算当中拥有更加高效的计算效率

但是sigmoid函数与tanh函数都拥有一个缺点——当输入数据的值比较大的时候，神经网络的学习速度会比较慢——因为学习速度与偏导数（斜率）的大小成正相关关系，而在sigmoid和tanh函数当中，当输入值越来越大的时候其斜率就越小

3.relu函数

![image-20210228143236637](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228143236.png)

![image-20210228141454142](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228141454.png)

于是出现了relu函数来解决这个问题，relu函数是最常用的激活函数，他是非线性的函数，可使用反向传播算法，当输入值小于0的的时其只会输出0，那么神经元就不会被激活，使得神经网络比较稀疏，对于计算来说比较有效率。

4.leaky relu

![image-20210228143248890](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228143249.png)

（当z<0的时候，0.01的值看自己的设置）

![image-20210228141851161](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228141851.png)

而为了解relu没有斜率的问题，leaky relu函数被创造出来，这个函数可以将0的梯度去掉，将输入为负值的情况下的函数斜率值变成一个很小但是却不为0的斜率。

5.softmax函数

softmax函数是另外一种sigmoid函数，但其是一种在分类当中比较容易控制的函数，softmax函数将输出结果压缩于0-1当中，并根据输出的总和进行分类，当你输出结果是【1.2,0.9,0.75,]时若此时使用softmax函数这个值变成了【0.42,0.31,0.27】我们可以使用这些概率当中每一类的概率



附：每一层神经网络当中都可以使用不同的激活函数，但是在实际开发当中不会每一层都使用不同的激活函数，一般来说，在激活函数的选择当中relu的选择是最多的（使用其一定要小心设置学习率learning rate），而如果relu不可以解决的话就考虑其他的函数，除了在二元分类的输出层以外，tanh都比sigmoid优秀。

#### 随机初始化参数

在多神经元神经网络当中，我们不可以将w和b默认为0，这会使得多神经元和单神经元无任何区别，我们一般使用numpy.random.randn() 进行初始化

![image-20210228144501008](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228144501.png)

实例：

![image-20210228144846553](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228144846.png)

![image-20210228144940415](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228144940.png)

![image-20210228144949389](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228144949.png)

* 选取一个合适的随机初始化区间

![image-20210228145037916](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210228145038.png)

同时，b是可以为0的，因为w已经被随机初始化了，每个神经元计算的内容已经不一样。

### 深层

* DNN模型概念

*刚刚的那些只是两层神经网络，而我们还可以构建三层六层乃至更多层的神经网络，而这些多层的神经网络就是深度神经网络，简称DNN，一般来说，我们把含有两层隐藏层的神经网络称为深层神经网络*

如图，以下就是一个DNN模型，该模型有4层，其中三层隐藏层，一层输出层，输入层不计![image-20210213165859893](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210213165900.png)

* DNN的使用原因

为什么非要使用DNN而不使用浅层神经网络呢，因为有些复杂的问题使用深层神经网络更加容易解决，而使用浅层神经网络可以解决，但是却会非常麻烦，相较于深层神经网络也会有更加多的神经元，运算复杂度也会更高，拿刚刚那个DNN模型来举例吧：

![image-20210213172453591](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210213172453.png)

而这些全部特征的提取都通过一个DNN模型来一次性的解决了，大幅度降低了运算复杂度，也大幅度的减轻了运算当中可能会出现问题的可能。

* DNN的迁移性

如上，我们可以知道，DNN的模型当中，每一层神经网络都有每一层的用处，那我们拥有一个训练好的神经网络模型之后，可以通过这个属性，稍微更改一下后面几层神经网络，就可以达到对其他识别任务的使用了，譬如说，人和猴子前面的特性都是一样的，都具有线条和器官，那我们可以通过更改后面几个有区别的层数来快速的达到识别猴子脸部的效果。

* DNN的计算
* 前向传播

![image-20210213183626850](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210213183626.png)

* 反向传播

![image-20210214153605746](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210214153605.png)

* 强调

![image-20210214153903846](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210214153903.png)

* 总览图

![image-20210214154516536](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210214154516.png)

![image-20210214154618891](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210214154618.png)

## 神经网络当中的矩阵维度

* 单个样本时

![image-20210215212732493](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210215212739.png)

![image-20210215212748348](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210215212748.png)

![image-20210215212802094](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210215212802.png)

* 多个样本时

![image-20210224091018492](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210224091025.png)

## 神经网络当中的参数与超参数

*这些参数可以影响神经网络的学习速度和最后的分类结果（神经网络的学习速度就是代价函数的成本值的下降快慢，而最后的分类结果就是分类正确率也就是预测准确率）*

![image-20210224093059227](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210224093059.png)

![image-20210226055602619](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210226055602.png)

每个超参数的选择都是不确定的，在每次更换神经网络或者在一个新环境下，之前的最优超参数都会有所变化，需要更改。

## 神经网络的分类

* 按照类别分类：监督学习和非监督学习（监督学习可以认为识别物体都有着自身标签在此基础上进行识别，现在创造价值的大部分神经网络都是该类神经网络）

* 按照结构分类：不同结构的神经网络被用于不同的应用场景

  ![image-20210226072753607](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210226072753.png)

![image-20210226072839019](https://gitee.com/ruomengawa/pic-go/raw/master/img/20210226072839.png)

## 深度学习三要素

1.数据

拥有足够多，足够准确的数据集，训练集是很重要的，相较于传统机器学习算法在不断增长的训练量当中训练性能会在某个量之后不再提升，而在神经网络当中，拥有足够的训练量才可以获得一个性能强大的神经网络。

2.计算力

拥有越来越强大的算力也是非常重要的，拥有强大的算力可以让我们在处理大规模的训练集时更加得心应手，而不至于每次训练一次模型就需要几天时间，而在这段时间一旦做出任何修改就需要全部重新来过。

3.算法

算法需要不断的创新，不断的优化，在相同的数据量和计算力下，拥有一个更好的算法可以使你更快完成训练，训练效果更好，在缩短训练周期的条件下，我们可以训练更大的神经网络，利用上更多的训练集。



